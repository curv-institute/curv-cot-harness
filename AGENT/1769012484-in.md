# Agent Prompt — Run Experiment 005 (Semantic Ambiguity Stress Tests)

Archive this prompt as:
* `AGENT/<UNIXTIME>-in.md`

---

## Objective

Run Experiment 005 to test whether trace injection induces agreement-on-wrong or collapse in tasks where correctness is semantic, ambiguous, or non-computable, rather than arithmetically verifiable.

This experiment explicitly targets the regimes where prior robustness results are not expected to hold.

---

## Single Experiment Axis (Mandatory)

**Axis:** task verifiability / ambiguity

Arithmetic verifiability → semantic plausibility

Everything else must remain fixed:
* Same proposer: Claude Sonnet 4.5
* Same target roster as Experiment 003/004
* Same prompts and decoding (temperature=0)
* Same corruption machinery (when enabled)

---

## Models

**Proposer (fixed)**
* Claude Sonnet 4.5

**Targets (unchanged)**
* Claude: Opus 4.5, Sonnet 4.5 (target), Haiku
* GPT (Codex CLI): gpt-5-codex, gpt-5.2-codex, gpt-5.2

Do not add or remove models in this experiment.

---

## Task Families (All Included)

You will evaluate four semantic task families. Each family is run independently but under the same experiment umbrella.

### Family A — Commonsense QA

Examples:
* Everyday physical reasoning
* Social expectations
* Intent inference

Characteristics:
* Often one "best" answer, but alternatives are plausible
* Weak internal verification

### Family B — Multi-step Causal / Story Reasoning

Examples:
* Short narratives with cause → effect chains
* Temporal reasoning

Characteristics:
* Errors propagate subtly
* Plausible but wrong explanations exist

### Family C — Distractor-heavy Word Problems (Non-numeric)

Examples:
* Irrelevant details
* Competing explanations

Characteristics:
* Salience-driven failure modes
* Agreement may form around a misleading frame

### Family D — Explanation-First Tasks

Examples:
* "Which explanation best accounts for…"
* Hypothesis selection

Characteristics:
* Sounding right matters more than being checkable
* High collapse risk

---

## Dataset Construction (Authoritative)

For each task family:
* Create N = 30 items (total 120 items)
* Items must have:
  * a designated gold answer
  * at least one plausible but wrong alternative

Datasets must be saved as:
```
eval/datasets/exp005_<family>.jsonl
```

Where `<family>` is one of:
* commonsense
* causal
* distractor
* explanation

Record source, generation method, or seed in a README next to each dataset file.

---

## Trace Conditions

Run three conditions per family:
1. No trace
2. Uncorrupted trace (normal proposer trace)
3. Corrupted trace (plausible semantic error)

Corruption must follow the same rules as Experiment 004:
* Deterministic item selection (hash-based)
* Minimal semantic error (one incorrect assumption or causal link)
* Plausible and fluent
* No self-identification of corruption

Save corrupted ids per family under:
```
eval/results/<run_name>/manifests/corrupted_ids_<family>.json
```

---

## Run Name

Use:
* `v0.1.4-semantic-ambiguity-stress-202601XX`

If run spans multiple days, keep the tag and vary only the date suffix.

---

## Required Outputs

All outputs live under:
```
eval/results/v0.1.4-semantic-ambiguity-stress-202601XX/
```

Required structure (per family):
```
<family>/
  prompts/
  traces/
    proposer_uncorrupted.jsonl
    proposer_corrupted.jsonl
  judgments/
    answers_no_trace.jsonl
    answers_with_uncorrupted_trace.jsonl
    answers_with_corrupted_trace.jsonl
  metrics.jsonl
  summary.json
```

Global files:
* manifest.json
* config.json
* report.md
* manifests/target_models_list.txt
* CLI version files

---

## Metrics (Required)

Compute per family and overall:
* Pairwise agreement (all three conditions)
* Agreement-on-correct
* Agreement-on-wrong
* Plurality consensus rate
* Plurality consensus strength
* Per-target accuracy
* Transfer gain vs no-trace

Stress-specific metrics:
* **Semantic collapse rate:**
  * fraction of items where plurality answer is wrong
* **Corruption-follow rate** (corrupted items only)
* **Disagreement suppression rate:**
  * how often trace reduces answer diversity while decreasing accuracy

---

## Report Requirements (Strict)

In `report.md`, include four short sections, one per task family, answering only:
1. Did trace injection increase agreement?
2. Did it increase accuracy?
3. Did agreement-on-wrong or plurality-wrong emerge?
4. Which models were most/least affected?

Then include a single comparison table across families.

No cross-family theorizing. No general claims.

---

## Version Control (Mandatory)

After verification:
```bash
jj commit
jj git push
```

Commit message (imperative):
* `Add Experiment 005 semantic ambiguity stress test results`

Archive:
* `AGENT/<UNIXTIME>-out.md` with per-family summaries and key tables.

---

## Stop Condition

Stop after push. Do not introduce harmonization, selectors, or verification layers unless a nonzero agreement-on-wrong or semantic collapse is observed.
