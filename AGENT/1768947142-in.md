# Agent Prompt — Run Experiment 002 (Claude-only multi-target baseline)

Archive this prompt as:
- AGENT/<UNIXTIME>-in.md

---

## Objective

Run Experiment 002 to make agreement and collapse metrics well-defined by using multiple target models.

This is a baseline run: no harmonization, no selector, no verifier coupling.

---

## Single Experiment Axis (Mandatory)

Axis: target model set size (single → multiple).

Everything else stays fixed versus Experiment 001:
- Same dataset: 60 arithmetic word problems
- Same proposer prompt and target prompts
- Deterministic decoding (temperature=0 or provider-equivalent)

---

## Models

**Proposer (fixed)**
- Claude Sonnet 4.5

**Targets (deterministic)**
- Claude Opus 4.5
- Claude Sonnet 4.5 (as a target)
- Claude Haiku

---

## Run Name

Use:
- v0.1.1-baseline-multitarget-claude-20260120

If running on a different date, update the suffix but keep the tag.

---

## Pre-Run Checks (Mandatory)

1. Confirm eval/datasets/arithmetic60.jsonl exists and is unchanged from Experiment 001.
2. Ensure CLI model calls are deterministic:
   - temperature = 0
   - top_p fixed (if available)
   - max tokens fixed (if available)
3. Update scripts/run_baseline.py run_cli() flags to match your installed Claude CLI.
4. Confirm the script still enforces JSON-only outputs and hard-fails on invalid JSON.

---

## Execution Command

Run the baseline runner with these targets.

Replace the placeholders with your actual CLI model IDs:

```bash
uv run scripts/run_baseline.py \
  v0.1.1-baseline-multitarget-claude-20260120 \
  eval/datasets/arithmetic60.jsonl \
  claude <SONNET_4_5_ID> \
  claude:<OPUS_4_5_ID>:opus_4_5 \
  claude:<SONNET_4_5_ID>:sonnet_4_5_tgt \
  claude:<HAIKU_ID>:haiku
```

---

## Required Outputs

Verify the following files exist under:
- eval/results/v0.1.1-baseline-multitarget-claude-20260120/

Required:
- manifest.json
- config.json
- prompts/ (3 prompt templates)
- traces/proposer.jsonl
- traces/injected.jsonl
- judgments/answers_with_trace.jsonl
- judgments/answers_no_trace.jsonl
- judgments/correctness.jsonl
- metrics.jsonl
- summary.json
- report.md

---

## Report Requirements (Scope-Limited)

In report.md, answer only:
1. Did trace injection increase pairwise agreement across targets?
2. Did trace injection increase per-target accuracy?
3. Did agreement-on-wrong (consistent-wrong) increase?

Do not speculate beyond the computed metrics.

---

## Version Control (Mandatory)

After the run completes and artifacts are verified:

```bash
jj commit
jj git push
```

Commit message (imperative):
- Add Experiment 002 multitarget baseline results

---

## Stop Condition

Stop after push is complete. Do not proceed to adding GPT targets or any harmonization operator unless instructed.
