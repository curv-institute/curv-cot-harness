# Experiment 003 Output — Claude + GPT-5.2 Heterogeneous Targets

## Run Summary

**Run name:** v0.1.2-baseline-multitarget-claude-plus-gpt52-20260120
**Dataset:** eval/datasets/arithmetic-60.jsonl (60 items)
**Proposer:** Claude Sonnet (100% accuracy)

**Targets (6 total):**
- Claude: opus, sonnet_tgt, haiku
- GPT: gpt-5-codex, gpt-5.2-codex, gpt-5.2

## Results

### Agreement
- Pairwise agreement rate (with trace): **0.9689**
- Pairwise agreement rate (no trace): **0.8889**
- Δ agreement: **+0.0800**

### Accuracy by Target Model

| Model | No Trace | With Trace | Transfer Gain |
|-------|----------|------------|---------------|
| opus | 1.0000 | 1.0000 | +0.0000 |
| sonnet_tgt | 1.0000 | 1.0000 | +0.0000 |
| haiku | 0.8833 | 0.9833 | +0.1000 |
| gpt5_codex | 0.9500 | 1.0000 | +0.0500 |
| gpt52_codex | 0.9500 | 0.9667 | +0.0167 |
| gpt52 | 0.8667 | 0.9500 | +0.0833 |

### All-Target Agreement Outcomes

| Condition | Agree-on-Correct | Agree-on-Wrong |
|-----------|------------------|----------------|
| With Trace | 55 | 0 |
| No Trace | 44 | 0 |

## Summary Questions (per experiment spec)

1. **Did trace injection increase pairwise agreement across all targets?**
   Yes (Δ = +8.0%, from 88.9% to 96.9%)

2. **Did trace injection increase per-target accuracy?**
   - Opus: No change (already 100%)
   - Sonnet: No change (already 100%)
   - Haiku: Yes (+10.0%)
   - gpt5_codex: Yes (+5.0%)
   - gpt52_codex: Yes (+1.67%)
   - gpt52: Yes (+8.33%)

3. **Did agreement-on-wrong (consistent-wrong) increase?**
   No (0 in both conditions)

## Key Observations

- Cross-family trace transfer works: Claude Sonnet traces improved GPT model accuracy
- Heterogeneous targets show larger agreement gains (+8%) vs Claude-only (+3.3% in Exp 002)
- All 4 weaker models (haiku, gpt5_codex, gpt52_codex, gpt52) benefited from traces
- No consistent-wrong collapse observed

## Commit

Commit `1a3b2244` pushed to origin/main.
