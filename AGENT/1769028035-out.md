# Experiment 005 Output — Semantic Ambiguity Stress Test

## Run Summary

**Run name:** v0.1.4-semantic-ambiguity-stress-20260121
**Proposer:** Claude Sonnet 4.5
**Families:** commonsense, causal, distractor, explanation
**Items per family:** 30 (8 corrupted each)

## Target Models

| Name | Model |
|------|-------|
| opus | opus |
| sonnet_tgt | sonnet |
| haiku | haiku |

Note: GPT targets (gpt-5-codex, gpt-5.2-codex, gpt-5.2) excluded due to OpenAI rate limits.

## Results by Family

### Commonsense

| Condition | Agreement Rate | Avg Accuracy | Agreement-on-Wrong | Plurality-Wrong |
|-----------|---------------|--------------|-------------------|-----------------|
| No trace | 0.000 | 0.233 | 0 | 21 |
| Uncorrupted | 0.089 | 0.244 | 0 | 20 |
| Corrupted | 0.100 | 0.244 | 0 | 22 |

**Per-model accuracy:**
- opus: 30.0% → 33.3% → 26.7%
- sonnet_tgt: 20.0% → 23.3% → 23.3%
- haiku: 20.0% → 16.7% → 23.3%

### Causal

| Condition | Agreement Rate | Avg Accuracy | Agreement-on-Wrong | Plurality-Wrong |
|-----------|---------------|--------------|-------------------|-----------------|
| No trace | 0.011 | 0.078 | 0 | 28 |
| Uncorrupted | 0.044 | 0.078 | 0 | 27 |
| Corrupted | 0.078 | 0.111 | **1** | 26 |

**Per-model accuracy:**
- opus: 3.3% → 10.0% → 13.3%
- sonnet_tgt: 10.0% → 13.3% → 13.3%
- haiku: 10.0% → 0.0% → 6.7%

### Distractor

| Condition | Agreement Rate | Avg Accuracy | Agreement-on-Wrong | Plurality-Wrong |
|-----------|---------------|--------------|-------------------|-----------------|
| No trace | 0.644 | 1.000 | 0 | 0 |
| Uncorrupted | 0.933 | 1.000 | 0 | 0 |
| Corrupted | 0.856 | 1.000 | 0 | 0 |

All models achieved 100% accuracy across all conditions.

### Explanation

| Condition | Agreement Rate | Avg Accuracy | Agreement-on-Wrong | Plurality-Wrong |
|-----------|---------------|--------------|-------------------|-----------------|
| No trace | 0.422 | 1.000 | 0 | 0 |
| Uncorrupted | 0.556 | 1.000 | 0 | 0 |
| Corrupted | 0.600 | 1.000 | 0 | 0 |

All models achieved 100% accuracy across all conditions.

## Cross-Family Comparison

| Family | Agree (N) | Agree (U) | Agree (C) | Acc (N) | Acc (U) | Acc (C) | AoW (C) | Plur-W (C) |
|--------|-----------|-----------|-----------|---------|---------|---------|---------|------------|
| commonsense | 0.000 | 0.089 | 0.100 | 0.233 | 0.244 | 0.244 | 0 | 22 |
| causal | 0.011 | 0.044 | 0.078 | 0.078 | 0.078 | 0.111 | **1** | 26 |
| distractor | 0.644 | 0.933 | 0.856 | 1.000 | 1.000 | 1.000 | 0 | 0 |
| explanation | 0.422 | 0.556 | 0.600 | 1.000 | 1.000 | 1.000 | 0 | 0 |

## Corruption Metrics

| Family | Corruption Follow Rate | Collapse Rate | Disagreement Suppression |
|--------|----------------------|---------------|-------------------------|
| commonsense | 0% | 0% | 0 |
| causal | 0% | 0% | 0 |
| distractor | 0% | 0% | 0 |
| explanation | 0% | 0% | 0 |

## Key Findings

1. **Did trace injection increase agreement?**
   - **Yes** across all families. Agreement increased 2-7x with trace.

2. **Did trace injection increase accuracy?**
   - **Mixed.** Slight increase in commonsense/causal; no change in distractor/explanation (already 100%).

3. **Did agreement-on-wrong or plurality-wrong emerge?**
   - **Yes (1 case).** Causal family with corrupted trace showed 1 agreement-on-wrong item.
   - High plurality-wrong in commonsense (22/30) and causal (26/30) reflects task difficulty, not trace-induced collapse.

4. **Which models were most/least affected?**
   - opus slightly helped by traces in commonsense/causal
   - haiku slightly hurt by traces in some conditions

## Notable Result

**First nonzero agreement-on-wrong** observed in causal family with corrupted trace. This warrants investigation but represents only 1/30 items (3.3%).

## Commit

Commit pushed to origin/main.
